\documentclass[oribibl]{llncs}

\usepackage{units}
\usepackage{psfrag} %% psfrac does not work with pdflatex
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{todonotes} %% add [disable] to disable
\usepackage{transparent}
\usepackage{pgfplots}

\usepackage{acronym} %% for abbreviations
%% physics related abbreviations
\acrodef{DFT}[DFT]{density functional theory}
\acrodef{XC}[XC]{exchange-correlation}
\acrodef{SHO}[SHO]{spherical harmonic oscillator}
\acrodef{DOS}[DoS]{density of states}
\acrodef{HO}[HO]{harmonic oscillator}
\acrodef{PW}[PW]{plane wave}
\acrodef{GP}[GP]{grid point}
\acrodef{AE}[AE]{all-electron}
\acrodef{PAW}[PAW]{Projector Augmented Wave}
%% computer science related abbreviations
\acrodef{CPU}[CPU]{central processing unit}
\acrodef{GPU}[GPU]{graphical processing unit}
\acrodef{HPC}[HPC]{high performance computing}
\acrodef{OMP}[OpenMP]{OpenMP}
\acrodef{MPI}[MPI]{message passing interface}


\setlength{\tabcolsep}{6pt}

\newcommand{\um}[1]{_{\mathrm{#1}}}
\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\lmax}{\ell_{\mathrm{max}}}
\newcommand{\ellm}{L}
\newcommand{\nrn}{n_{\mathrm{r}}}
\newcommand{\ket}[1]{\left| #1 \right\rangle}
\newcommand{\bra}[1]{\left\langle #1 \right|}
\newcommand{\braket}[2]{\left\langle \left. #1 \right| #2 \right\rangle}
\newcommand{\braketop}[3]{\left\langle \left. #1 \right| #2 \left| #3 \right. \right\rangle}
\newcommand{\codename}{A4cube}

\begin{document}
\pagestyle{plain}

\title       {\codename{} - User Manual}
\titlerunning{\codename{}}

\author{%
  Paul F.~Baumeister\inst{1} % \and %
}

\institute{%
  J\"{u}lich Supercomputing Centre, Forschungszentrum J\"{u}lich, 52425 J\"{u}lich, Germany
%   \and Institute for Advanced Simulation, Forschungszentrum J\"{u}lich, 52425 J\"{u}lich, Germany
}

\maketitle

\begin{figure*}
	\centering
	\includegraphics[width=3cm]{fig/a43_logo_bold_no_helper_lines} %%
\end{figure*}

% ==============================================================================
\begin{abstract}
This is a user manual for everyone who is interested in using
this software.
As far as possible, all aspects of programming are omitted here.
However, some aspects related to the algorithms that the applications 
makes use of will need be covered in order to explain the handling.
\end{abstract}
% ==============================================================================

\section*{How to read this document?}
\todo[inline]{Write}

\newpage
% ==============================================================================
\section{Introduction} \label{sec:intro}
% ==============================================================================
%
\subsection{Density Functional Theory}
\codename{} is an implementation for performing calculations in the framework of \ac{DFT}.
A fair number of books and a vast number of articles in the scientific 
literature is on \ac{DFT}, therefore, we will only give a brief recap 
of the very basics needed to understand what kind of physics can be
simulated with \codename{} and also, where the limitations of predictive power
are.

\ac{DFT} is a ground state method, this means that in principle we have to
assume zero Kelvin. However, you will see later that for special situations 
finite temperatures are applied to smear out the Fermi-Dirac distribution 
for electronic occupation numbers in order to improve convergence.
In principle, all results should be extrapolated to \unit[0]{K}.


The underlying Hohenberg-Kohn theorem tells us, that we can derive some 
ground state properties of the system once we have found the correct density.
In order to find the density, Kohn and Sham introduced single-particle wave functions $\Psi_i$, which are eigenstates of an effective single-particle Hamiltonian $\hat H$.
With a little simplification, we can write the total electron density
\begin{equation}
	n(\vec r) = \sum_i^{\mathrm{occ}} \left| \Psi_i(\vec r) \right|^2
	\label{eqn:simplified_density_generation}
\end{equation} 
As a function or functional of the density $n(\vec r)$ we can construct
a local effective potential entering the Hamiltonian
\begin{equation}
	\hat H\um{Kohn-Sham} = \frac{\left( \imath \hbar \vec \nabla \right)^2}{2 m_e} + V\um{eff}(\vec r)
	\label{eqn:Kohn-Sham_Hamiltonian_no_spin}
\end{equation} 
where $V\um{eff}(\vec r)$ includes
\begin{itemize}
  \item the attractive Coulomb potential of the atomic nuclei,
  \item a repulsive Hartree potential from electrostatic 
  		interaction of an electron with the density,
  \item a contribution accounting for exchange and electronic correlation
  \item and potential external fields.
\end{itemize}

\subsection{The Exchange-Correlation Functional}
The true many-body wave function for a system with electrons (as being Fermions)
is anti-symmetric under exchange of two particles. 
This implies that e.g.~a two electrons wave function $\Psi(\vec r_1,\vec r_2)$ must be zero whenever $\vec r_1 \rightarrow \vec r_2$ in order to obey
the anti-symmetry constraint.
This lowers the repulsion energy of a real two-electrons system compared to a
system of independent charged particles.
In \ac{DFT} the construction of independent single-particle states is an approximation that requires to model the exchange interaction.

Furthermore, there are correlation effects of true many-body electron systems
which also cannot be easily captured in standard \ac{DFT}.
Both together are modelled in the contribution $V\um{xc}(\vec r)$ 
to the total effective potential and a corresponding energy density $\epsilon\um{xc}$ for the evaluation of a total energy.

Different parametrizations of the \ac{XC} functional are available.
\codename{} focusses onto the three classes of local (non-orbital dependent) functionals: 
\begin{itemize}
	\item Local (Spin) Density Approximations, LDA/LSDA, 
	\item Generalized Gradient Approximations, GGA
	\item Meta-GGAs
\end{itemize}
LDAs generate $V(\vec r)$ only from the value of $n(\vec r)$, 
hence the name \emph{local}. 
GGAs include some gradients of the density, $|\vec \nabla n(\vec r)|$.
Still, the spatial argument $\vec r$ is the same but thinking for example
in terms of finite-difference derivatives on a grid, 
a little non-local character of GGAs becomes visible.
Finally, meta-GGAs incorporate information about the kinetic energy density.
In analogy to eq.~(\ref{eqn:simplified_density_generation}), the kinetic energy density $\tau$ can be found by
\begin{equation}
	\tau(\vec r) = \frac{\hbar^2}{2m_e} \sum_i^{\mathrm{occ}} \braketop{ \Psi_i } {\vec \nabla^2 } { \Psi_i } 
	\label{eqn:simplified_kinetic_energy_density_generation}
\end{equation}
where we introduced Dirac's \emph{bra-ket} notation.
Currently, only the 
%Perdew-Zunger'81 \ttt{PZ81}
Perdew-Wang'91 \ttt{PW91}-LDA is implemented
but it is planned to interface \ttt{libxc} with access to most existing functional.
\todo[inline]{Implement LIBXC, reference for LIBXC}



\section{How to get started}

\subsection{Getting the Code}
\codename{} will be public domain software under the MIT license
and will be available to clone or download at \ttt{github.com/real-space/A43}. 
\todo[inline]{get GitHub account}

\subsection{Building the Application} \label{sec:compiling}
A \ttt{Makefile} is contained in the program package that works with current GNU compilers, e.g.~\ttt{g++ 7.4.0}.
\todo[inline]{get CMake to work}
%% and \ttt{nvcc from CUDA 10}

The application \codename{}, while being written in C++, relies strongly on plane-old-data types,
i.e.~classes without constructors or destructors as known from C-structs.
Furthermore, it makes use of generic programming in the form of templates
mainly used to maintain a single function supporting different precision (\ttt{float} or \ttt{double}), 
difference vector lengths, \ttt{real} and \ttt{complex} and in some situations different physics.
This helps to keep the code base small and, hence, maintainable.

\section{Basic Handling}
\codename{} comes as a single executable named \ttt{a43} which
offers many functionalities accessible by options. Type \ttt{./a43 -h}
to get a first help function:
\begin{verbatim}
Usage ./a43 [OPTION]
   -h, -H       This help message
   -t <module>  Run module unit test
   +<var>=<val> Modify variable environment
\end{verbatim}

An important functionality after building the application (see section \ref{sec:compiling}) 
is to run the unit tests of the available modules.
In order to get a list of available modules, use 
\begin{verbatim}
./a43 -t '?'
\end{verbatim}
where the high quotes are necessary so your shell does not try to interpret the question mark.
Pass the name of one of the listed modules to run a module unit test like this:
\begin{verbatim}
./a43 -t complex_tools
\end{verbatim}
We can also run the unit test of all modules, however, for this we might want to reduce
the amount of output to \ttt{stdout}.
We may use the variable environment 
(mind: this \codename{} internal variable environment is not linked to that of the shell) 
which couples to the command line interface with a leading \ttt{+}. For example use
\begin{verbatim}
./a43 +verbosity=1 -t
\end{verbatim}
to change the \ttt{verbosity} to minimal when running all unit tests.

\todo[inline]{Ensure that all modules have some meaningful unit tests or at least say it that they don't}

The default \ttt{verbosity}-level is $3$ and levels are adjusted frequently during development.
However, the ground rule says that higher \ttt{verbosity} means more output
and \ttt{verbosity=0} suppresses all output except for fatal errors.
We can also use the options \ttt{--verbose} to set \ttt{verbosity=6} (high)
or fine tune the verbosity level by adding +$1$ for every \ttt{-v} and +$4$ for every \ttt{-V}.
Table \ref{tab:verbosity-level-meaning} gives a rough overview on verbosity levels. 

\begin{table}[ht!]
\caption[Meaning of verbosity levels]{
Meaning of verbosity levels
\todo[inline]{Is this table needed?}
} \label{tab:verbosity-level-meaning}
\centering
\begin{tabular}{|r|l|}
\hline
  \ttt{verbosity} & Meaning   \\
\hline
     0 & No output whatsoever \\
     1 & Minimal output       \\
     3 & Default low          \\
   5-7 & High detail          \\
 $>$ 8 & Debugging            \\
\hline
\end{tabular}
\end{table}


\subsection{Atomic Units, Input and Output}
Internally \codename{} makes use of Hartree atomic units:
\begin{equation}
	\hbar = m_e = e = 1
\end{equation}
as this reduces the number of non-trivial factors in many equations.
This means that the internal unit of length is Bohr = \unit[0.529177]{\AA{}}
and the internal unit of energy is Hartree = \unit[27.2114]{$e$V}.
However, we can change the length and energy units used for output like this:
\begin{verbatim}
./a43 +output.energy.unit=eV +output.length.unit=Ang ...
\end{verbatim}
%% sigma_config uses Bohr always (as input and output unit) on purpose
Available output units are \ttt{eV}, \ttt{Ry} or \ttt{Ha} (default in the development branch) for energies 
and \ttt{nm}, \ttt{Ang} (=\AA) and \ttt{Bohr} (default) for lengths. Run
\begin{verbatim}
./a43 -t unit_system
\end{verbatim}
to verify this.
Input length units are \AA{} in the geometry file as this is most convenient for molecular structure viewers
(\ttt{VESTA}, \ttt{jmol}, etc.). Other input quantities may have flexible input units, for example
\begin{verbatim}
./a43 +logder.unit=Ry +logder.step=1e-3
\end{verbatim}
which means that the energy spacing for checking of the logarithmic derivatives is $10^{-3}$Rydberg.
The same input unit is assumed for \ttt{+logder.start} and \ttt{+logder.stop}.


Experimental parts of the code may not support the flexible unit conversion
factors for output. Then you should be able to find at least indications as \ttt{Ha}, \ttt{Bohr} or simply \ttt{a.u.} in the log files.
Most standard functionality, however, supports unit conversion so that log files
show \ttt{eV} or \ttt{Ry} and \ttt{Ang}$ = $\AA{} or \ttt{nm}, depending on the users choice.
Some quantities are displayed in characteristic units, e.g., plane wave cutoffs are typically shown in Rydberg.


\subsection{Try a first structure}
Make sure there is structure file \ttt{atoms.xyz} file and run
\begin{verbatim}
./a43 -t geometry_analysis
\end{verbatim}
in order to check if your geometry looks ok.
You can search the log file for warnings using \ttt{grep} but
there will also be a summary of all warnings launched during the execution at the end of the log file.
Once free of warnings, continue with the next step:
Let's say we want aluminum in our system, then run:
\begin{verbatim}
./a43 -t single_atom +single_atom.test.Z=13
\end{verbatim}
\todo[inline]{Currently, aluminum produces a critical charge deficit operator eigenvalues}


\codename{} will the try to access a database of initial atomic potentials
(full spherically, spin-degenerate) which are stored in \ttt{pot/Zeff.}$Z$
where $Z$ is a 3-digit integer atomic number.
\todo[inline]{make the path to pot/ configurable}
You can even add \ttt{+single\_atom.init.scf.maxit=33} to let the atomic density relax if e.g.~the \ac{XC} functional differs from the one that was used to generate the \ttt{Zeff.}-file.
If the loading fails because of a missing file, you can bootstrap it:
\begin{verbatim}
./a43 -t atom_core +atom_core.test.Z=13
\end{verbatim}
or if the entire database was lost,
\begin{verbatim}
./a43 -t atom_core \
        +atom_core.test.Z=1 +atom_core.test.Z.end=86
\end{verbatim}
This may take some minutes. To track the progress, \ttt{verbosity=5} is recommended.
\todo[inline]{OpenMP would be good here}

Now, run the \ttt{single\_atom} test for each of the elements in the system to see if there are warnings before you continue like this
\begin{verbatim}
./a43 -t potential_generator \
        +geometry.file=molecule.xyz
\end{verbatim}
\todo[inline]{we have not yet defined a functionality of main! only tests so far}

For the advanced user, we recommend to inspect the logarithmic derivatives by
\begin{verbatim}
./a43 -t single_atom +single_atom.test.Z=13 \
        +logder.start=-4 +logder.step=1e-4 +logder.stop=1.0
\end{verbatim}
The output can be visualized by \ttt{gnuplot} or \ttt{xmgrace -nxy}.
For the ease of use, we may add \ttt{+logder.unit=eV} and then specify
\ttt{start}, \ttt{stop} and \ttt{step} in units of $e$Volt.


\subsection{Parallel Computing}
Currently, only the \ttt{geometry\_analysis} part of \codename{} supports shared memory parallelism 
via \ttt{omp} threads on the \ac{CPU}.
In order to target current \ac{HPC} systems 
the goal is to incorporate the \ac{MPI}
for inter-node communication
and the \ttt{CUDA} programming model for acceleration by a \ac{GPU}.
\todo[inline]{MPI parallelize}
\todo[inline]{Import CUDA finite-difference from ReSpawN}
\todo[inline]{Import CUDA sho-transform from ReSpawN}
\todo[inline]{Import CUDA tfqmr from tfQMRgpu}

Please consult with your \ac{HPC} system administrator for support
about how to control the different levels of parallelism.

\section{Representation of Wave Functions}

Kohn-Sham eigenstates have very different character depending on their energy level: core states, valence states and depending on the species also semicore states.
\subsubsection{Treatment of Core States}
Core states are very low in energy and strongly bound 
to the atomic core.
Their spatial extent is so strongly localized 
in the vicinity of the atomic nucleus 
that they feel almost exclusively the strong attractive 
(effective because screened) Coulomb potential of the core. 
This can be approximated fairly well as a spherically symmetric potential.
with a singularity at its origin.
The eigenstates show a clear $\ell$-character, 
i.e.~$s$, $p$, $d$ or even $f$ states can be found.
Technically, the solutions are found on a 1D radial grid,
where also relativistic effects can be treated properly.
The energy is well defined and degenerate with respect to
the magnetic quantum number $m$.

\subsubsection{Treatment of Semicore States}
Semicore states are high-lying core states
but energy-wise below the valence band.
Their wave function tail can extend beyond half a nearest neighbor atom distance.
In a tight-binding picture, the hoppings to the neighboring atom are still small
but the non-sphericality of the potential becomes important.
The $m$-degeneracy of the energy (found for core states) breaks 
and crystal field splittings or even bands with small dispersion can be observed.
\todo[inline]{Implement semicores}
It is planned to implement semicore states with a separate energy window
and either tight-binding like derived from atomic orbitals 
(eigenfunctions of the spherically averaged potential) 
or with a full \ac{PAW} treatment as follows for the valence states.

\subsubsection{Treatment of Valence States}
Valence states are high in energy, by definition up to the Fermi
energy, are delocalized over several atoms and form bands and bonds.
They are particularly difficult to describe because of the 
inhomgeneiety of features:
Valence states exhibit relatively slow oscillations 
in the space between atoms (interstitial space) but close to the atomic nuclei,
their structure resembles that of high-lying core states.
In particular projected radial coordinates, we can find up to $6$ nodes
(7$s$-bands) with the distance between the nodes becoming smaller
the closer we move towards the core.
These rapid oscillations are difficult to represent
on the same footing that gets the behaviour in the interstitial regions correct. 
We will review some of the most common approaches:

Local orbital methods use radial eigenfunctions 
of the spherically averaged atomic potential as atom-centered basis functions.
This captures the behaviour in the core region well
but shows a slow and not systematic convergence behaviour in the interstitial region. 
An advantage is that atomic orbital basis sets can be rather compact
and localized which casts the Hamiltonian into a block-sparse matrix operator with small block dimensions ($\approx 30$).
The drawback is that the description in the interstitial region determines the
quality of description for bandstructures, bonds and forces.

LAPW (Linearized Augmented Plane Wave) methods take the best of two worlds:
In order to construct efficient basis functions, 
they assume a spherical potential inside an atomic sphere (solutions are radial)
and a constant potential inside the interstitial region (solutions are plane waves). 
On the boundary spheres between the two regions, the basis function components need to be matched.
Due to the \ac{PW}-character, the basis functions are fully delocalized and 
the Hamiltonian becomes dense.

Using \ac{PW}s as a basis for the entire space shifts the problem to the representation of the valence wave function in the core region.
The \ac{PAW} method is a successful generalization of the family of pseudopotential methods with allows a full-potential access.
A pseudopotential replaces the true deep full potential in the core region
by some potential which can be represented with a finite number of \ac{PW}s
and approximates the scattering properties (phase shifts) in the typical energy
range of valence states.
In most cases the construction of such a potential only succeeds in the form of
a non-local potential.

Real-space grid based methods for \ac{DFT}
feature an improved scalablity on massively parallel \ac{HPC} systems at the same level of accuracy as \ac{PW} basis sets. 
The \ac{PAW} method can be utilized and comes at the advantage that the projector functions are localized around the atoms, 
hence, the Hamiltonian becomes an implicit low-rank operator with sparsity which does not involve Fourier transforms when applied to a wave function.


\codename{} implements Cartesian real-space grids 
in combination with a revision of the \ac{PAW} method (revPAW) suitable for
a reduced memory capacity consumption and lower memory bandwidth requirements.
\todo[inline]{cite Baumeister, Tsukamoto, PASC19 proceedings}


% ==============================================================================
\bibliographystyle{splncs03} \bibliography{literature}
% ==============================================================================

\end{document}
